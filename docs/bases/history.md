---
hide:
  - toc
---

# Des cailloux à l'informatique
Avant de voir comment programmer un ordinateur, revenons un moment sur la raison pour laquelle et la façon dont les ordinateurs sont apparus. Cela nous aidera pour la suite de cette initiation.

## Notion de nombres et calculs
Dans le règne animal, la **numérosité** semble assez répandue. Chez l'Homme comme chez le chien, un système cognitif a pu être mis en évidence lors d'études (non invasives) sur l'activité cérébrale : le **système de nombres approximatifs**. Ce système permet, pour simplifier, d'estimer "à la louche" la quantité d'éléments dans un groupe. Il n'implique aucun symbole pour représenter les nombres. C'est grâce à lui qu'un chien pourra choisir la gamelle contenant "le plus" de croquettes. Il n'y a donc pas de comptage précis, juste une estimation sur la quantité et permet une comparaison des quantités entre différents lots.

On pense donc que les premiers Hommes ne savaient pas compter. Au mieux il pouvait avoir une idée sur des quantités comme l'unité ou la multitude, voire mesurer grossièrement différents niveaux de multiplicité ("un seul", "quelques uns", "beaucoup"). Cependant, la nécessité de compter est apparue avec les besoins.

Le mot **calcul** vient du latin _calculus_ signifiant "caillou". Plusieurs origines peuvent expliquer ce lien entre le calcul mathématique et les cailloux. Une légende veut que le berger plaçait un petit caillou dans un pot pour chaque mouton qui sortait de la bergerie. Il retirait les cailloux un à un pour chaque mouton qui y rentrait. Il pouvait alors savoir si un ou des moutons n'étaient pas revenus.<br/>
De la même manière, et avant l'apparition des nombres romains, les soldats romains  pouvaient lancer un caillou dans une urne avant de partir au combat puis récupérait un caillou du récipient en revenant. Le nombre de cailloux restant servaient à compter combien de soldats étaient tombés lors de l'affrontement.

Les premiers calculs se sont donc faits sur des **nombres entiers** pour dénombrer des soldats dans une armée, des animaux dans un troupeau ou pour des transactions financières. Les nombres décimaux apparaîtront plus tard dans l'histoire de l'Humanité. Le premier outil pour compter et faire des calculs est évidemment **la main**. Certaines cultures se sont basées sur les dix doigts de la main, donnant naissance à la base 10 que nous utilisons encore de nos jours grâce au système numérique arabe utilisé en Occident. D'autres civilisations comme celle des aztèques se sont basées sur les doigts des mains et des pieds ce qui leur permettait d'utiliser une base 20.

Pour rappel, une base arithmétique _N_ définit le nombre de chiffres disponibles allant de 0 à _N_-1. Pour aller au-delà de la valeur du plus grand chiffre, il faut ajouter une puissance.<br/>
Concrètement, prenons l'exemple de la base 10 que nous utilisons quotidiennement. Nous avons dix chiffres - dix, comme la base donc - allant de 0 à 9. Puis, pour passer à la valeur après 9, nous ajoutons une puissance : la _dizaine_. On passe alors de 9 à 10, un nombre constitué de deux chiffres. Nous restons à deux chiffres jusqu'à 99 où, pour passer à la valeur suivante, nous ajoutons encore une puissance : la _centaine_. Voici le schéma que nous obtenons :
```text
   0    1    2    3    4    5    6    7    8    9
  10   11   12   13   14   15   16   17   18   19 [...]   90  91  92  93  94  95  96  97  98  99
 100  101  102  103  104  105  106  107  108  109 [...]  990 991 992 993 994 995 996 997 998 999
1000 1001 1002 1003 1004 1005 1006 ....
```

Cependant la main a un défaut : elle ne permet pas de conserver le résultat intermédiaire d'un calcul. Il est donc nécessaire de s'en souvenir ce qui n'est pas toujours une mince affaire quand on manipule des nombres mentalement. Les cailloux ou encore les bâtons de marquage ont bien sûr été un premier moyen de "stocker" un résultat comme nous l'avons vu pour les moutons et les soldats, ou aussi pour des marchandises. Cependant, ces moyens ont certaines limitations dont le nombre nécessaire pour représenter de grandes valeurs. Par ailleurs, s'il est facile et rapide de compter de petites quantités de cailloux ou d'entailles sur un bâton, il devient beaucoup plus difficile de compter un grand nombre d'entre eux.

## Les premières machines à calculer analogiques
Avec les calculs sur des nombres plus grands, il est donc devenu nécessaire d'inventer des outils d'aide au calcul. Les plus connus sont l'**abaque** et le **boulier** (2000 av J.-C.). Plus tard, au premier siècle avant Jésus Christ, la **machine d'Anticythère** permet de calculer à l'avance la date et l'heure des éclipses solaires et lunaires pour les siècles à venir. D'autres outils de calcul astronomique ont été inventés durant les siècles suivants comme l'**astrolabe grec** ou le **planisphère de Al-Bīrūnī** au XIème siècle.<br/>
En 1642, Blaise Pascal présente sa **pascaline**, une machine à calculer mécanique numérique permettant d'effectuer des additions, soustractions directement, ainsi que des multiplications et divisions par répétitions. Pascal l'avait conçue à l'origine pour aider son père, nouvellement surintendant de Haute-Normandie, dans la lourde tâche de remettre d'aplomb les recettes fiscales de cette province. Malgré la praticité de la machine, son prix élevé ne lui a pas permis de se démocratiser. La pascaline fût donc, hélas, un échec commercial. Bien que Pascal voulait corriger sa première pascaline en la simplifiant et donc la rendre moins chère, un accident de carrosse l'a totalement détourné de la recherche scientifique. Il a alors abandonné son projet pour se consacrer à la philosophie et la religion.

## Prémices de l'automatisation
En 1725, Basile Bouchon programme un métier à tisser par la **lecture d'un ruban perforé**, inspiré des boîtes à musiques, pour automatiser la tâche répétitive du tissage. C'est la première fois qu'une machine semi-automatique est utilisée à des fins industrielles. L'idée sera ensuite perfectionnée sur plusieurs itérations jusqu'au succès mondial du **métier à tisser de Jacquard**.

## Ada Lovelace et la machine analytique
Le métier à tisser de Jacquard inspire **Charles Babbage**. En 1833, celui-ci propose une machine à calculer programmable encore mécanique, la **machine analytique**. La complexité et les changements de plan à répétition avortent le projet du calculateur. Cependant, entre 1842 et 1843, **Ada Lovelace** qui collaborait avec Charles Babbage commençait à concevoir des cartes perforées pour cette machine, autrement dit les premiers programmes en **langage binaire** (présence ou absence de trou). **Ada Lovelace est donc la première programmeuse de l'Histoire de l'Humanité, tous sexes confondus**.<br/>
Plus tard, les avancées sur la connaissance sur l'électricité permettent de remplacer certaines parties mécaniques des calculateurs. On parle alors d'_électromécanique_.

A la fin du XIXème siècle, le métier à tisser de Jacquard est utilisé en combinaison avec les cartes perforées par Herman Hellorith, pour effectuer le recensement de la population américaine. Hellorith fonde la Tabulating Machine Company en 1896, qui fusionnera en 1911 avec Computing Scale Company. Cette fusion donne alors naissance à une nouvelle entreprise, la Computing- Tabulating- Recording Co. En 1924 elle est renommée en _International Business Machine Corporation_, mieux connue sous le sigle **IBM**.

## La Seconde Guerre Mondiale et les débuts de l'informatique
Au début du XXème siècle, avant la Seconde Guerre Mondiale, les **calculateurs analogiques** servaient à résoudre des équations différentielles, notamment grâce à leur capacité à faire des intégrations. Les valeurs d'entrée étaient des grandeurs physiques (température, tension, etc, _par analogie_) et donnaient en retour des valeurs chiffrées plus ou moins précises. Ces calculateurs ont le gros inconvénient d'être spécifiques à un problème donné et doivent alors être **reprogrammés manuellement** pour résoudre un autre problème. Le numérique s'est peu à peu imposé, notamment grâce à sa programmabilité.

A cette époque, et jusqu'au milieu du XXème siècle, les calculateurs et les premiers ordinateurs fonctionnaient à l'aide de **tubes à vide** après l'invention de la triode en 1906. Pour décrire brièvement le fonctionnement de tels tubes, un corps conducteur servant de cathode (électrode négative) est chauffé à très haute température. Cela permet aux électrons de s'en détacher. Les électrons sont des particules chargées négativement et leur déplacement génère un courant électrique. Les électrons parcourent du vide ou un gaz inerte en sens unique jusqu'à rencontrer un autre corps conducteur, l'anode (électrode positive). Ainsi, si on fait suffisamment chauffer la cathode, on peut propager un courant électrique. Ce courant électrique sert ensuite dans un **langage binaire : il y a du courant ou il n'y en a pas**. Les tubes à vide présentent plusieurs inconvénients, d'une part l'importante consommation d'énergie mais aussi leur fiabilité médiocre. Il n'était pas rare que des tubes grillent occasionnant des pannes sur les machines en fonctionnement.<br/>
Les tubes à vide sont cependant encore utilisés comme dans les fours à micro-ondes. Ils présentent aussi l'intérêt de ne pas être sensibles aux impulsions électromagnétiques.

La Seconde Guerre Mondiale est un événement majeur y compris dans l'histoire de l'informatique. Les communications écrites étaient transmises par téléscripteur en **code Baudot**, un ancien code plus vieux que l'ASCII. Le code Baudot permettait de représenter chaque caratère (lettre, chiffre, ponctuation) sur 5 bits. Le bit ne pouvant prendre que deux valeurs, 0 ou 1, le code Baudot n'offrait que 32 combinaisons différentes et donc pas assez pour représenter les 26 lettres de l'alphabet, les 10 chiffres et d'autres caractères comme les sauts de ligne. Deux codes binaires étaient alors dédiés au passage du mode "alphabet" au mode "chiffre". Les codes suivants étaient alors interprétés soit comme une lettre, soit comme un chiffre, soit comme un caractère spécial suivant le mode utilisé.<br/>
Cependant, en temps de guerre, les communications écrites n'étaient pas transmises en clair. Autrement dit, pour éviter que l'ennemi ne puisse lire les communications, elles étaient chiffrées et, _théoriquement_, seul le destinataire de la communication pouvait les déchiffrer. Un chiffrement connu, bien que très basique, est celui qu'utilisait Jules César qui décalait les lettres dans l'alphabet. Depuis, la cryptographie s'est considérablement améliorée pour éviter que les chiffrements ne soient cassés trop facilement. Durant la Seconde Guerre Mondiale, les téléscripteurs envoyaient donc des messages chiffrés. On peut par exemple évoquer les machines de Lorenz utilisées par l'Allemagne nazie pour chiffrer ses communications. Le Colossus était un **ordinateur numérique** construit très secrètement entre 1942 et 1943 à Londres, et servait à la cryptanalyse du code Lorenz pour pouvoir déchiffrer les communications allemandes. Le Colossus n'était pas le seul ordinateur, numérique ou électromécanique, à avoir été développé durant le conflit.

L'un de ces ordinateurs qui est encore très célèbre n'est autre que l'**ENIAC**, pour _Electronic Numerical Integrator And Computer_. Il est développé pour le calcul balistique dans un premier temps puis sert aux problèmes de physique nucléaire et de météorologie. Il s'agit d'une énorme machine pesant 30 tonnes et occupant une salle entière avec une consommation estimée à environ 150kW ou plus.<br/>
L'ENIAC est une machine qu'on programme par câblage. Cela fait que le calculateur ne travaille pas durant de longs moments, le temps qu'il soit programmé. Le mathématicien John von Neumann comprend qu'il s'agit là d'un frein important pour exploiter tout le potentiel de la machine. En 1946, il publie un rapport qui décrit l'**architecture dite _de von Neumann_**, le modèle de ce qu'on appelle un **ordinateur**. Cette architecture comprend plusieurs éléments :

- Une **unité arithmétique et logique**, aussi appellée ALU en anglais, pour effectuer des opérations de base (addition, soustraction, multiplication, division et des opérations binaires) ;
- Une **unité de contrôle** qui sert à donner la succession des instructions à l'ALU ;
- De la **mémoire interne** pour y stocker le programme, c'est-à-dire l'ensemble des instructions, et les données. Cette mémoire est le coeur même de l'idée de von Neumann pour améliorer les performances de la machine ;
- Et des **dispositifs d'entrée et de sortie** qui permettent à la machine d'interagir avec l'être humain.

En 1948, l'ENIAC sera amélioré avec une mémoire en lecture seule pour stocker le programme et les informations. De fait, il devient _Turing-complet_, d'après l'idée de la **machine universelle du mathématicien et cryptologue Alan Turing**.

## Qu'est-ce que la mémoire informatique ?
Le rapport de John von Neumann constitue à la fois un véritable bon en avant pour l'informatique mais ouvre en contrepartie la question de savoir ce qu'est la mémoire d'une machine. Plusieurs idées ont émergé pour concevoir une **mémoire vive**, comme des lignes de délai au mercure. Parmi les défauts de cette mémoire, elle ne permettait pas un accès aléatoire aux données. Le but d'une **mémoire à accès aléatoire** (_Random Access Memory_ ou RAM en anglais) est de **pouvoir accéder directement à une information quelque soit son emplacement**. Une mémoire séquentielle comme une bande magnétique nécessite de parcourir toute la mémoire jusqu'à l'emplacement voulu, et donc fait perdre énormément de temps. Un exemple assez parlant est celui de la K7 audio qui nécessite de faire défiler la bande jusqu'au début d'une chanson. Au contraire, un CD permet un accès direct au début d'une chanson.

Le premier prototype d'ordinateur basé sur l'architecture de von Neumann voit le jour en 1948. C'est le **Small-Scale Experimental Machine** (SSEM), construit à l'université Victoria de Manchester. La mémoire vive emploie un tube de Williams, un tube cathodique stockant chaque bit sous la forme de point (0) ou de trait (1). Le SSEM n'a pas de réelle utilité calculatoire, il se limite à des opérations mathématiques sans grande importance mais suffisamment longues pour effectuer l'expérimentation. Le but de cette machine est de vérifier le bon fonctionnement d'un ordinateur tel que décrit par von Neumann. Le SSEM montre bien la faisabilité du concept de von Neumann, ce qui ouvre la voie à la construction de plusieurs gros ordinateurs au début des années 1950 à travers le monde.

## L'avènement des ordinateurs
Les calculateurs analogiques et les ordinateurs coexisteront durant quelques décennies. En effet, les ordinateurs sont encore très chers et leur fonctionnement numérique, différent, n'est pas adapté à tous les problèmes rencontrés. Cependant, les grands calculateurs analogiques tomberont en désuétude autour des années 80. Toutefois, un regain d'intérêt pour certains calculs s'opère avec des calculateurs analogiques/numériques.<br/>
Les premiers ordinateurs sont développés dans des laboratoires universitaires mais très vite, l'industrie s'y intéresse. Par exemple, IBM produit l'IBM 701 de 1952 à 1954 pour le calcul scientifique et l'IBM 702 en 1953, moins performant, à usage commercial. Ce dernier utilise un stockage sous forme de dérouleur de bandes magnétiques pour concurrencer l'UNIVAC I. En 1953 toujours, IBM lance l'IBM 650 avec une mémoire de masse magnétique à tambour, une technologie concurrente aux disques durs magnétiques.

Un tournant important dans l'histoire des ordinateurs s'effectue à la fin des années 50. En effet, la _première génération_ d'ordinateurs, comme les calculateurs analogiques, utilisent des tubes à vide pour les unités de calcul. Ces tubes sont encombrants, consomment énormément d'énergie mais sont aussi très peu fiables provoquant d'innombrables pannes. Le **transistor** est inventé en 1947 par **John Bardeen, William Shockley et Walter Brattain** alors qu'ils voulaient amplifier les communications téléphoniques longue distance en remplaçant les tubes à vide trop fragiles.

Le transistor est un dispositif semi-conducteur, c'est-à-dire qu'on peut contrôler s'il laisse passer ou non les électrons. Le transistor peut ainsi être vu comme un interrupteur électronique. Quelques années après sa découverte, le transistor est utilisé commercialement avec les premiers postes dits à transistors. C'est encore un peu après qu'il vient remplacer le tube à vide dans les ordinateurs, leur permettant notamment de consommer moins, d'être plus fiables et plus compacts. En 1954, Bell Labs produit le TRADIC, le tout premier ordinateur à transistors qui en contenait 700.

En 1958, Jack Kilby, nouvellement employé chez Texas Instruments, invente le **circuit intégré** permettant la miniaturisation et faisant rentrer l'informatique dans l'ère moderne. Il recevra d'ailleurs le prix Nobel de physique en 2000 pour cette invention. Kilby a commencé par prototyper un tel circuit en reliant à la main les transistors avant que l'idée soit industrialisée. Elle donnera les boîtiers contenant des centaines de transistors, enfermant en son sein tout ce qui permet de réaliser des calculs et de la mémoire, et accessibles par des éléments d'entrées et sorties à l'aide de pattes en périphérie. La première utilisation des ordinateurs basés sur des circuits intégrés, légers et fiables, est faite avec le programme de missiles ballistiques Minuteman II, puis le célèbre programme de missions Apollo pour la conquête de la Lune.

Les programmes militaires et spatiaux sont parmi ceux, si ce ne sont ceux, qui font le plus avancer la science et la technologie. Bien qu'il soit difficile de départager lesquelles ont le plus démocratiser l'informatique entre les missions ballistiques et spatiales, elles y ont toutes contribué. La conception de fusée par exemple nécessite une perfection absolue dans l'usinage des pièces en métallurgie, ce qui aidera à la précision lors de la fabrication de composants électroniques. De meme les instruments de mesure doivent être extrêmement fiables et précis. La gestion de projet s'est aussi considérablement développée et améliorée pour devenir ce qu'elle est aujourd'hui dans tous les domaines.
Le programme Apollo a par ailleurs permis l'essor de l'informatique et l'a scindé en deux pôles : le matériel (_hardware_ en anglais) et le logiciel (_software_ en anglais). Outre la fiabilité du matériel, la politique de test des logiciels s'est aussi développée pour éviter les défaillances des programmes.

Le circuit intégré a permis de miniaturiser les éléments d'un ordinateur. Cependant, les différents composants sont des entités séparées occasionnant des lenteurs et des problèmes liés à la fiabilité dans la connexion entre ces composants. C'est en 1971 que ces problèmes sont corrigés car sort le tout premier microprocesseur, le Intel 4004, un processeur 4 bits. **En ne mesurant pas plus de 11mm², il a une puissance de calcul équivalente à celle de l'ENIAC**. Il est suivi l'année suivant du 8008 qui peut traiter jusqu'à 8 bits simultanément. Le processeur mythique d'Intel sortira en 1974, le Intel 8080 lui aussi en 8 bits. Il aura de nombreux concurrents comme le Zilog Z80 qui était d'ailleurs compatible avec le processeur d'Intel, ou encore le Motorola 6800. L'Intel 8080 a été amélioré les années suivantes donnant par exemple le 8085.

Les **premiers ordinateurs personnels grand public** sortent en 1977. Il s'agit de :

* l'Apple II avec un processeur MOS Technology 6502 8 bit (1 MHz) et 4 ko de RAM (jusqu'à 64 ko),
* le TRS-80 (Tandy RadioShack) équipé d'un Zilog Z80 (1.77 MHz) et 4 ko de RAM.
* et le Commodore PET 2001 doté d'un MOS Technology 6502 8 bit (1 MHz) et 4 ou 8 ko de RAM, vendu pour l'équivalent de 3555 dollars en 2021.

Il n'est évidemment pas possible de parler de l'entièreté des microprocesseurs ayant existé et de leurs spécificités. La suite de cet historique se concentre essentiellement sur ceux des marques qui sont les plus connues à l'heure actuelle.

Le Intel 8086 sorti en 1978 est le premier processeur 16 bits et sera aussi le tout premier processeur de la **famille x86**, la plus répandue de nos jours pour les ordinateurs personnels, serveurs et stations de travail. Pour son 80286, Intel a dû demander à une autre entreprise de produire des clones de ses processeurs car IBM exigeait de ses clients deux sources d'approvionnement. C'est ainsi qu'**AMD** a commencé à produire l'Am286 en 1982, bien que totalement conçu par Intel. AMD continuera de produire des processeurs pour Intel pendant quelques années. Le Intel 80386 publié en 1986 est le premier microprocesseur 32 bits et implémente l'architecture IA-32 (Intel Architecture 32 bits) qu'on retrouve partout de nos jours. Intel assure la rétrocompatibilité des nouveaux processeurs avec les anciens afin que les programmes déjà écrits puissent continuer à fonctionner. A noter qu'à cette époque encore, pour des raisons techniques et de coût, les microprocesseurs n'intègrent pas d'unité de calcul en virgule flottante (_floating point unit_ ou _FPU_ en anglais), c'est-à-dire pour réaliser des opérations sur des nombres à virgule. Ils doivent donc être couplés à des coprocesseurs dédiés à de tels calculs pour y parvenir. Il faudra attendre le 80486 d'Intel sorti en 1989 pour voir le premier microprocesseur intégrant - en option - une FPU.

En 1993 sortent les Intel Pentium. Ils intègrent comme nouveauté un jeu d'_instructions vectorielles_ appelé MMX permettant de traiter plusieurs données simultanément en une seule instruction. Cela accroît les performances des processeurs sur les applications multimédia. Intel réutilisera sur plusieurs années la marque Pentium pour ses processeurs grand public ainsi que leur jumeaux dédiés aux professionnels sous la marque Xeon. En 1995, **AMD introduit son premier processeur x86 indépendemment d'Intel**, l'AMD-K5. La concurrence entre Intel et AMD a donc commencé. L'argument de vente en ce temps était la fréquence. Les premiers processeurs à passer le cap symbolique du GHz sont l'Athlon (AMD) et le Pentium III (Intel). Avec son Xeon Northwood, Intel introduit l'**hyper-threading** qui duplique certains éléments du cœur du processeur comme les registres. Cette technologie permet d'améliorer en théorie les performances d'applications dites multithreadées, éxecutant plusieurs instructions en parallèle. Les Pentium 4 hériteront de l'hyper-threading mais leur microarchitecture conçue pour la course à la fréquence entraîne une consommation et une dissipation de chaleur très importantes. La course à la fréquence a donc atteint ses limites... Intel et AMD changent leur fusil d'épaule pour partir à la conquête du nombre de cœurs. Alors que l'AMD Athlon 64 sorti en 2003 est le tout premier processeur 64 bits (tout en étant compatible avec les programmes 32 bits), l'AMD Athlon X2 et le Pentium Extreme Edition sont, eux, les premiers modèles dotés de deux cœurs en 2005. Ce dernier est complété par de l'hyper-threading donnant un total de deux cœurs physiques et quatres processeurs logiques. Les performances des processeur multicœurs font un bon en avant alors que leur fréquence est plus faible. Les années qui suivent jusqu'à nos jours voient le nombre de cœurs toujours plus grand chez Intel et AMD.

Au fil du temps, la finesse de gravure s'est considérablement réduite. Elle correspond à la taille des transistors composant les microprocesseurs. Plus ces transistors sont petits, plus il est possible d'en avoir dans un processeur. Le Intel 4004 était gravé avec une finesse de 10µm (soit 10000nm) et était composé de 2300 transistors. Trente ans plus tard, en 2001, le Pentium III Tualatin avait une finesse de gravure de 130nm et possédait 45 millions de transistors. Le marketing a énormément pris le relai sur la taille réelle des transistors. Les chiffres annoncés de 5nm pour les processeurs les plus récents ne reflètent plus la réalité grâce à des astuces ingénieuses comme des transistors en 3D.

En marge des deux leaders sur les microarchitectures IA-32 puis AMD64/IA-64, une autre société développe des architectures de processeurs 32 et 64 bits. Il s'agit de ARM, dont il n'est pas possible d'acheter un microprocesseur seul : ils sont intégrés dans des systèmes sur puce (_system on a chip_ ou _SoC_ en anglais). On retrouve les SoC ARM dans de nombreux produits, en particulier mobiles (téléphones et smartphones, ordinateurs et consoles portables, tablettes, etc), car les architectures _arm_ sont simples et optimisent la consommation électrique. Cependant, on retrouve également des supercalculateurs équipés de SoC conçus par ARM, comme le supercalculateur japonais Fugaku. En 2020, Apple annonce le M1, une puce ARM 64 bits qui équipe ses nouveaux produits tels que les tablettes, ordinateurs mobiles et de bureau. En 2021, Apple lance les puces M1 Pro et M1 Max encore plus puissantes et le M2 sort en 2022 plaçant la barre toujours plus haute.

Il existe également des microcontrôleurs, ou MCU pour _microcontroller unit_ en anglais, qu'on retrouve dans énormément d'appareils électroniques dans des systèmes embarqués. Les MCU sont également des circuits intégrés rassemblant un processeur, de la mémoire (vive et morte), des entrées et sorties, etc. Souvent dédiés à des tâches spécifiques, ils sont très peu chers car leurs capacités en calculs sont limitées ; ils consomment peu avec une fréquence faible et leurs spécificités limitées font qu'ils ne sont pas aussi polyvalents que les microprocesseurs des ordinateurs personnels.

Maintenant que cet historique nous a conduit jusqu'à quelques technologies de nos processeurs modernes, je te propose de voir comment ces derniers fonctionnent.


??? info "Sources"

    - https://hmn.wiki/fr/Approximate_number_system
    - https://www.numerama.com/sciences/594706-les-nombres-activent-la-meme-zone-du-cerveau-chez-votre-chien-que-chez-vous.html
    - https://www.larousse.fr/dictionnaires/francais/informatique/42996
    - https://www.franceculture.fr/emissions/la-methode-scientifique/quand-lhumanite-t-elle-appris-compter
    - http://revue.sesamath.net/spip.php?article891
    - https://www.youtube.com/watch?v=gLzHlA33rqw
    - https://journals.openedition.org/bibnum/548
    - https://www.historyofinformation.com/detail.php?id=4727
    - https://www.youtube.com/watch?v=-2604CHuIyk
    - http://claude-gimenes.fr/electronique/semi-conducteurs/-vii-tube-a-vide-electronique-ancetre-du-transistor-amplificateur
    - https://www.futura-sciences.com/tech/actualites/technologie-tubes-vide-futur-nanoelectronique-40058/
    - https://aconit.inria.fr/omeka/exhibits/show/histoire-machines.1.html
    - https://www.ibm.com/ibm/history/history/history_intro.html
    - https://www.youtube.com/watch?v=dcN9QXxmRqk
    - https://www.techno-science.net/definition/2400.html
    - http://www.numdam.org/article/ASCFM_1962__8_2_131_0.pdf
    - https://gallica.bnf.fr/ark:/12148/bpt6k24678x/f535.item (_Les merveilles de la science, ou Description populaire des inventions modernes. 5-6, Suppléments. 5 par Louis Figuier, Chapitre IV - LE TÉLÉGRAPHE BAUDOT, A TRANSMISSION MULTIPLE._ ark:/12148/bpt6k24678x)
    - https://www.futura-sciences.com/sciences/questions-reponses/mathematiques-quest-ce-chiffrement-cesar-8032/
    - https://www.britannica.com/technology/ENIAC
    - https://www.universalis.fr/encyclopedie/e-n-i-a-c/
    - https://www.histoire-informatique.org/grandes_dates/2_3.html
    - https://www.futura-sciences.com/sciences/personnalites/matiere-john-von-neumann-256/
    - https://history-computer.com/ssem-complete-history-of-the-small-scale-experimental-machine/
    - https://info.blaisepascal.fr/transistor
    - https://www.cairn.info/revue-le-temps-des-medias-2004-2-page-118.htm
    - https://www.digikey.fr/fr/articles/transistor-basics
    - https://couleur-science.eu/?d=775902--cest-quoi-un-transistor-comment-ca-marche
    - https://www.irif.fr/~carton/Enseignement/Architecture/Cours/Historic/tradic.html
    - https://www.lmd.jussieu.fr/~hourdin/COURS/FORTRAN/Fortran/
    - https://www.webtimemedias.com/article/le-deces-de-jack-kilby-inventeur-du-circuit-integre
